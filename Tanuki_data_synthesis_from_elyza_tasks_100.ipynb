{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaage0703/colab-notebooks/blob/main/Tanuki_data_synthesis_from_elyza_tasks_100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9FLfpec3mkN"
      },
      "source": [
        "# データ合成\n",
        "\n",
        "## 参考\n",
        "- https://note.com/npaka/n/ndbbd243f8073\n",
        "- https://github.com/axolotl-ai-cloud/axolotl/issues/911\n",
        "- https://colab.research.google.com/drive/114CRa-xh0yh-jVzzSav2VL3vqBOHSgr9?hl=ja#scrollTo=Rqy-n7Fi9mDb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "セットアップ\n",
        "\n",
        "Google Colabのシークレットキーに`HF_TOKEN`という名前でHugging Faceのトークンを追加しておいてください。"
      ],
      "metadata": {
        "id": "F_1tAtKkUFOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "# シークレットキーを使わない場合は以下に直接入力してください\n",
        "# HF_TOKEN = \"xxx\""
      ],
      "metadata": {
        "id": "qatCv3SjUEcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ライブラリセットアップ"
      ],
      "metadata": {
        "id": "oKYBfo3akxxp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOrr-Ujp3sXP"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq -U transformers accelerate bitsandbytes\n",
        "!pip install -qqq flash_attn --no-build-isolation\n",
        "#!pip install flash-attn==2.3.3\n",
        "!pip install -qqq autoawq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpVcO4kTNlfk"
      },
      "outputs": [],
      "source": [
        "!pip install pyarrow==14.0.1\n",
        "!pip install torch==2.4.0 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanuki-8x8Bのセットアップ"
      ],
      "metadata": {
        "id": "XXCW8UgC19tx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bquzq1j4OvG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
        "\n",
        "model_name = \"team-hatakeyama-phase2/Tanuki-8x8B-dpo-v1.0-AWQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name\n",
        ")\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanuki-8x8Bのテスト"
      ],
      "metadata": {
        "id": "MvoA4Dqok1l7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WZ5bSkm4R2E"
      },
      "outputs": [],
      "source": [
        "# メッセージの準備\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"あなたはAIアシスタントです。\"},\n",
        "    {\"role\": \"user\", \"content\": \"こんにちは\"}\n",
        "]\n",
        "\n",
        "# 推論の実行\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.5,\n",
        "    #streamer=streamer\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの変換"
      ],
      "metadata": {
        "id": "A7taPwHVqqtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELYZA-tasks-100のロード"
      ],
      "metadata": {
        "id": "Hh7THojt2JSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset(\"elyza/ELYZA-tasks-100\")"
      ],
      "metadata": {
        "id": "KHybrJdfpvQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SC7rdZMruUER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "metadata": {
        "id": "FJoGbgWRuUyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "システムプロンプト設定"
      ],
      "metadata": {
        "id": "FhVF_HculTor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = r\"\"\"\n",
        "あなたには試験の練習問題をつくってもらいたいです。\n",
        "\n",
        "プロンプトには input, outputが含まれていて、 inputには質問、outputには答えが入っています。\n",
        "これと似たような問題が出題されるので、練習問題として input, outputを参考に、新しい質問(input)と答え(output)のセットを1つ作成してください。\n",
        "inputとoutputは、元の問題とは違う、新しいものにしてください。\n",
        "\n",
        "出力形式は以下としてください。\n",
        "\n",
        "input: ###\n",
        "{input}\n",
        "###\n",
        "\n",
        "output: ###\n",
        "{output}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "ACZVXLQ01_Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データセットからプロンプトを生成"
      ],
      "metadata": {
        "id": "HosrR9jolY3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "prompt_list = []\n",
        "\n",
        "for dt in tqdm(datasets[\"test\"]):\n",
        "    prompt = f\"input: ###\\n{dt['input']}\\n###\\n\\noutput: ###\\n{dt['output']}\\n###\"\n",
        "    prompt_list.append(prompt)"
      ],
      "metadata": {
        "id": "Dqh7-w6P2W4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ確認"
      ],
      "metadata": {
        "id": "RzUtcSoplfzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_list[0]"
      ],
      "metadata": {
        "id": "Y2qfyhAQ1VYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_prompt_list = prompt_list[:2]"
      ],
      "metadata": {
        "id": "fJXFL3bAry5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, prompt in enumerate(tqdm(prompt_list), start=1):\n",
        "#for i, prompt in enumerate(tqdm(test_prompt_list), start=1):\n",
        "    print(f\"number: {i}\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # 推論の実行\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        temperature=1.0,\n",
        "    )\n",
        "\n",
        "    #text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    text = tokenizer.decode(output_ids.tolist()[0][input_ids.size(1) :], skip_special_tokens=True)\n",
        "    input_text = \"\"\n",
        "    output_text = \"\"\n",
        "\n",
        "    try:\n",
        "        input_text = text.split(\"input: ###\")[1].split(\"###\")[0].strip()\n",
        "        output_text = text.split(\"output: ###\")[1].split(\"###\")[0].strip()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    # 結果を辞書形式で保存\n",
        "    results.append({\"id\": i, \"text\": text, \"input\": input_text, \"output\": output_text})"
      ],
      "metadata": {
        "id": "_c6jpTo6mPER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[0]"
      ],
      "metadata": {
        "id": "g9DAZYXZsKQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_excel(\"elyza_tasks_100_new.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "kArpOp7T088m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOuDvqFO7LKvN6SWCYKzpWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}